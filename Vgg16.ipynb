import numpy as np
import pandas as pd
import tensorflow
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping
#calls to save the model or weights
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
from tensorflow.keras.models import Model
train_dir = '/content/drive/MyDrive/dataset/archive ....zip (Unzipped Files)/archive/tomato/train'
validation_dir = '/content/drive/MyDrive/dataset/archive ....zip (Unzipped Files)/archive/tomato/val'
test_dir = '/content/drive/MyDrive/dataset/archive ....zip (Unzipped Files)/archive/tomato/test'
from google.colab import drive
drive.mount('/content/drive')
#input images has height,width and 3 RGB channels
image_size = 224
#Load the VGG model .
from tensorflow.keras.applications import VGG16
vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))
#indicate that all the layers in the VGG16 model are not to be trained again. You only want to directly use the parameter.
for layer in vgg_conv.layers[:]:
    layer.trainable = False
# It will check trainable status individual
for layer in vgg_conv.layers:
    print(layer, layer.trainable)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
model = Sequential()
# Add the vgg convolutional base model
model.add(vgg_conv)
#Add new layers
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))
## Add activation function to classifier
model.add(Dense(10, activation='softmax'))
model.summary()
#Data augmentation
from tensorflow.keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(rescale=1./255)
batchSize = 32
train_generator = datagen.flow_from_directory(train_dir, 
                                             target_size=(image_size, image_size), 
                                             batch_size=batchSize, 
                                             class_mode='categorical', shuffle=True)

validation_generator = datagen.flow_from_directory(validation_dir, 
                                                   target_size=(image_size, image_size),
                                                   batch_size=batchSize,
                                                   class_mode='categorical', shuffle=False)

test_generator = datagen.flow_from_directory(test_dir, 
                                             target_size=(image_size, image_size), 
                                             batch_size=batchSize, 
                                             class_mode='categorical', shuffle=False)

id = 0
print('The shape of train images: {}'.format(train_generator[id][0][0].shape))
plt.imshow(train_generator[id][0][0])
plt.axis('off')
train_image_label_id = np.argmax(train_generator[id][1][0])
classes_list = list(train_generator.class_indices.keys())
plt.title('Class name: {}'.format(classes_list[train_image_label_id]))
plt.show()
print(classes_list)
id = 0
print('The shape of validation images: {}'.format(validation_generator[id][0][0].shape))
plt.imshow(validation_generator[id][0][0])
plt.axis('off')
val_image_label_id = np.argmax(validation_generator[id][1][0])
classes_list = list(validation_generator.class_indices.keys())
plt.title('Class name: {}'.format(classes_list[val_image_label_id]))
plt.show()
id = 0
print('The shape of test images: {}'.format(test_generator[id][0][0].shape))
plt.imshow(test_generator[id][0][0])
plt.axis('off')
tes_image_label_id = np.argmax(test_generator[id][1][0])
classes_list = list(test_generator.class_indices.keys())
plt.title('Class name: {}'.format(classes_list[tes_image_label_id]))
plt.show()
#Compiling the Model
from tensorflow.keras import optimizers
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
# early stopping training a neural network is to decide how many epochs a model should be trained.
callback = tf.keras.callbacks.EarlyStopping(
    monitor='Val_loss', patience=5)
 #This callback will stop the training when there is no improvement in
# the loss for three consecutive epochs.
model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
##Trainig the network
history = model.fit(train_generator, steps_per_epoch=train_generator.samples//train_generator.batch_size, epochs=25, validation_data=validation_generator, 
                    validation_steps=validation_generator.samples//validation_generator.batch_size,callbacks=[callback], verbose=1)
                   
#loss
plt.figure(figsize=[8, 6])
plt.plot(history.history['loss'], 'r', linewidth=3.0)
plt.plot(history.history['val_loss'], 'b', linewidth=3.0)
plt.legend(['Training loss', 'Validation Loss'], fontsize=18)
plt.xlabel('Epochs', fontsize=16)
plt.ylabel('Loss', fontsize=16)
plt.title('Loss Curves', fontsize=16)
plt.show()
#accuracy
plt.figure(figsize=[8, 6])
plt.plot(history.history['accuracy'], 'r', linewidth=3.0)
plt.plot(history.history['val_accuracy'], 'b', linewidth=3.0)
plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)
plt.xlabel('Epochs', fontsize=16)
plt.ylabel('Accuracy', fontsize=16)
plt.title('Accuracy Curves', fontsize=16)
plt.show()
model.save('/content/drive/MyDrive/Colab Notebooks/myModel.h5')
from keras.models import load_model
model = model.load_model = load_model('/content/drive/MyDrive/Colab Notebooks/myModel.h5')
#Getting filenames from generator
fnames = test_generator.filenames
# Get ground truth from generator
ground_truth = test_generator.classes
#get the label to class mapping from generator
label2index = test_generator.class_indices
#obtain the list of the classes
idx2label = dict((v,k) for k,v in label2index.items())
# get the predictions from the model using the generator
predictions = model.predict(test_generator, steps=(test_generator.samples//test_generator.batch_size)+1, verbose=1)
#get the class index to class label
predicted_classes = np.argmax(predictions, axis=1)
errors = np.where(predicted_classes != ground_truth)[0]
print('No of errors = {}/{}'.format(len(errors), test_generator.samples))
#show the errors
from tensorflow.keras.preprocessing.image import load_img
for i in range(len(errors)):
    pred_class = np.argmax(predictions[errors[i]])
    pred_label = idx2label[pred_class]
    print('Original label: {}, Prediction: {}, Confidence: {:.3f}'.format(fnames[errors[i]].split('/')[0], pred_label, predictions[errors[i]][pred_class]))
    original = load_img('{}/{}'.format(test_dir, fnames[errors[i]]))
    plt.imshow(original)
    plt.show()
    #load an image
from tensorflow.keras.preprocessing.image import img_to_array
filename = '/content/drive/MyDrive/dataset/archive ....zip (Unzipped Files)/archive/tomato/test/Tomato___Bacterial_spot/01a3cf3f-94c1-44d5-8972-8c509d62558e___GCREC_Bact.Sp 3396.JPG'
original = load_img(filename, target_size=(224, 224))
# numpy image height, width, channel
numpy_image = img_to_array(original)
#convert the image into batch format
#expand_dims will add an extra dimension to the data at a particular axis
#we want the input matrix to the network to be the form(batchsize, height, width, channels)
image_batch = np.expand_dims(numpy_image, axis=0)
plt.imshow(np.uint8(image_batch[0]))
plt.show()
prediction = np.argmax(model.predict(image_batch), axis=1)
probability = model.predict(image_batch)
predicted_class = np.argmax(probability[0])
predict_label = idx2label[predicted_class]
print('Prediction: {}'.format(predict_label))
model.evaluate(test_generator)
